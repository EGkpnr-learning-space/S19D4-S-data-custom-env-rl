{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§± Ã–zel Ortam\n",
    "\n",
    "---\n",
    "\n",
    "Bu gÃ¶revde, derste Ã¶rnek olarak kullanÄ±lan **Ä±zgara tabanlÄ± navigasyon ortamÄ±nÄ±n** birebir aynÄ±sÄ±nÄ± tasarlayÄ±p uygulayacaksÄ±nÄ±z.\n",
    "\n",
    "- Ajan Ä±zgaranÄ±n bir kÃ¶ÅŸesinde baÅŸlar.\n",
    "- AmaÃ§ karÅŸÄ± kÃ¶ÅŸeye ulaÅŸmaktÄ±r.\n",
    "- Izgarada ajanÄ±n kaÃ§Ä±nmasÄ± gereken engeller veya \"delikler\" bulunabilir.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ AmaÃ§lar\n",
    "\n",
    "- ğŸ“ **OrtamÄ± TanÄ±mlayÄ±n**: ajanÄ±n baÅŸlangÄ±Ã§tan hedefe doÄŸru hareket ettiÄŸi bir Ä±zgara dÃ¼nyasÄ± oluÅŸturun.\n",
    "- âš™ï¸ **Ortam Dinamiklerini UygulayÄ±n**: hareket ve Ã¶dÃ¼l atamasÄ± kurallarÄ±nÄ± programlayÄ±n (Ã¶rn. deliklere dÃ¼ÅŸmek iÃ§in ceza, hedefe ulaÅŸmak iÃ§in Ã¶dÃ¼l).\n",
    "- ğŸ‘ï¸ **GÃ¶zlem ve Eylemleri AyarlayÄ±n**:\n",
    "    - **GÃ¶zlem alanÄ±** â†’ AjanÄ±n algÄ±ladÄ±ÄŸÄ± ÅŸeyler (Ã¶rn. Ä±zgaradaki pozisyonu)  \n",
    "    - **Eylem alanÄ±** â†’ AjanÄ±n yapabileceÄŸi ÅŸeyler (Ã¶rn. yukarÄ±, aÅŸaÄŸÄ±, sola, saÄŸa hareket)\n",
    "- ğŸ–¼ï¸ **Render Metodu Ekleyin**: ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in `.render()` fonksiyonu ekleyin â€” hata ayÄ±klama ve ajanÄ±n davranÄ±ÅŸÄ±nÄ± anlama iÃ§in faydalÄ±dÄ±r.\n",
    "- ğŸ§© **Ã–zel Ã–zellikler Ekleyin**: derste ele alÄ±nan engeller, Ã§ukurlar veya diÄŸer Ã¶zellikleri dahil ederek ortamÄ±nÄ±zÄ± daha dinamik ve gerÃ§ekÃ§i hale getirin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bu gÃ¶rev iÃ§in ihtiyacÄ±mÄ±z olan tÃ¼m paketleri iÃ§e aktararak baÅŸlayalÄ±m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ§© BÃ¶lÃ¼m 1: Ã–zel Ortam SÄ±nÄ±fÄ± OluÅŸturma\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, kÄ±smen yazÄ±lmÄ±ÅŸ bir sÄ±nÄ±fÄ± tamamlayarak Ã¶zel ortamÄ±nÄ±zÄ± uygulayacaksÄ±nÄ±z.  \n",
    "YapÄ±, Gymnasium'un ortam oluÅŸturma iÃ§in standart formatÄ±nÄ± takip eder.\n",
    "\n",
    "### ğŸ› ï¸ YapacaklarÄ±nÄ±z\n",
    "\n",
    "- BazÄ± temel metotlarÄ± Ã¶nceden yazÄ±lmÄ±ÅŸ bir ÅŸablon saÄŸlanmÄ±ÅŸtÄ±r.  \n",
    "- `# CODE HERE` yazdÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z her yerde, eksik mantÄ±ÄŸÄ± doldurmanÄ±z gerekir.  \n",
    "- Her adÄ±mda size rehberlik etmesi iÃ§in satÄ±r iÃ§i ipuÃ§larÄ± verilmiÅŸtir.\n",
    "\n",
    "### ğŸ¯ Hedefiniz\n",
    "\n",
    "- SÄ±nÄ±fÄ± gerÃ§ek bir Gymnasium ortamÄ± gibi davranacak ÅŸekilde tamamlayÄ±n.  \n",
    "- ÅunlarÄ± yapmalÄ±:\n",
    "  - GeÃ§erli bir gÃ¶zlem ve eylem alanÄ± tanÄ±mlamalÄ±  \n",
    "  - Ajan hareketini ve geÃ§iÅŸleri ele almalÄ±  \n",
    "  - Ã–dÃ¼lleri dÃ¶ndÃ¼rmeli ve `done` bayraklarÄ±nÄ± uygun ÅŸekilde gÃ¼ncellemeli  \n",
    "  - Ã‡alÄ±ÅŸan bir `.reset()` ve `.step()` metodu iÃ§ermeli  \n",
    "  - Ä°steÄŸe baÄŸlÄ± olarak gÃ¶rselleÅŸtirme iÃ§in basit bir `.render()` iÃ§ermeli\n",
    "\n",
    "ğŸ§  Her metodu anlamak iÃ§in zaman ayÄ±rÄ±n â€” Ã¶zellikle durum geÃ§iÅŸlerinin ve Ã¶dÃ¼llerin nasÄ±l yÃ¶netildiÄŸini. RL ortamlarÄ± burada canlanÄ±r.\n",
    "\n",
    "ğŸ“š Ortam yapÄ±sÄ± ve en iyi uygulamalar hakkÄ±nda ayrÄ±ntÄ±lÄ± adÄ±mlar iÃ§in resmi [Gymnasium Ã¶zel ortam kÄ±lavuzuna](https://gymnasium.farama.org/introduction/create_custom_env/) baÅŸvurun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGridEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.size = # Size of the square grid\n",
    "        # self.action_space = # Define the action space to have four possible actions: right, up, left, down\n",
    "        # self.agent_position = # Initial position of the agent (top-left corner)\n",
    "        # self.goal_position =  # Position of the target (bottom-right corner)\n",
    "        # self.hole_position =  # Position of the hole (bottom middle)\n",
    "        # self.action_to_direction = # Mapping from actions to grid movements\n",
    "        # 1. OrtamÄ±n Boyutu ve DeÄŸiÅŸkenleri\n",
    "        self.size = 3  # 3x3 bir Ä±zgara\n",
    "        \n",
    "        # 2. Eylem AlanÄ± (Action Space): 4 hareket (SaÄŸ, YukarÄ±, Sol, AÅŸaÄŸÄ±)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # 3. Pozisyonlar\n",
    "        self.agent_position = np.array([0, 0], dtype=np.int32) # BaÅŸlangÄ±Ã§ (Sol Ãœst)\n",
    "        self.goal_position = np.array([2, 2], dtype=np.int32)  # Hedef (SaÄŸ Alt)\n",
    "        self.hole_position = np.array([2, 1], dtype=np.int32)  # Delik (Alt Orta)\n",
    "\n",
    "        # 4. Hareket HaritasÄ± (Eylemleri koordinat deÄŸiÅŸimine Ã§evirir)\n",
    "        # [satÄ±r_deÄŸiÅŸimi, sÃ¼tun_deÄŸiÅŸimi] formatÄ±nda\n",
    "        self.action_to_direction = {\n",
    "            0: np.array([0, 1]),   # SaÄŸ\n",
    "            1: np.array([-1, 0]),  # YukarÄ±\n",
    "            2: np.array([0, -1]),  # Sol\n",
    "            3: np.array([1, 0]),   # AÅŸaÄŸÄ±\n",
    "        }\n",
    "\n",
    "        # Define the observation space using dictionary spaces for agent and target locations\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(low=0, high=self.size - 1, shape=(2,), dtype=np.int32),\n",
    "            \"target\": spaces.Box(low=0, high=self.size - 1, shape=(2,), dtype=np.int32),\n",
    "        })\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict, Dict]:\n",
    "        # Reset the environment for a new episode\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # self.agent_position = # Reset the agent's position to the starting position\n",
    "        # AjanÄ± baÅŸlangÄ±Ã§ noktasÄ±na geri koy\n",
    "        self.agent_position = np.array([0, 0], dtype=np.int32)\n",
    "\n",
    "        # Goal position remains constant\n",
    "        self.goal_position = np.array([2, 2], dtype=np.int32)\n",
    "\n",
    "        # Generate observation and info for the current state\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Return current observation of agent and target positions\n",
    "        return {\n",
    "            \"agent\": self.agent_position.copy(),\n",
    "            \"target\": self.goal_position.copy()\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        # Compute the Manhattan distance between the agent and the target\n",
    "        distance = np.sum(np.abs(self.agent_position - self.goal_position))\n",
    "        return {\"distance\": distance}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action to change the agent_position\n",
    "        # If agent fell in hole, give -10 reward, and set done=True\n",
    "        # elif agent reached goal, give +10 reward, and set done=True\n",
    "        # else, give -1 reward, and set done=False\n",
    "        # 1. Yeni pozisyonu hesapla\n",
    "        direction = self.action_to_direction[action]\n",
    "        \n",
    "        # Mevcut pozisyona yÃ¶nÃ¼ ekle\n",
    "        new_position = self.agent_position + direction\n",
    "\n",
    "        # 2. Izgara sÄ±nÄ±rlarÄ±nÄ± kontrol et (Duvara Ã§arpma)\n",
    "        # np.clip, deÄŸerlerin 0 ile (size-1) arasÄ±nda kalmasÄ±nÄ± saÄŸlar\n",
    "        self.agent_position = np.clip(new_position, 0, self.size - 1)\n",
    "\n",
    "        # 3. Ã–dÃ¼l ve BitiÅŸ DurumlarÄ±nÄ± Kontrol Et\n",
    "        \n",
    "        # Ajan deliÄŸe dÃ¼ÅŸtÃ¼ mÃ¼?\n",
    "        if np.array_equal(self.agent_position, self.hole_position):\n",
    "            reward = -10\n",
    "            done = True\n",
    "        \n",
    "        # Ajan hedefe ulaÅŸtÄ± mÄ±?\n",
    "        elif np.array_equal(self.agent_position, self.goal_position):\n",
    "            reward = 10\n",
    "            done = True\n",
    "        \n",
    "        # BoÅŸ bir kareye adÄ±m attÄ±\n",
    "        else:\n",
    "            reward = -1  # Her adÄ±m iÃ§in kÃ¼Ã§Ã¼k ceza (en kÄ±sa yolu bulmasÄ± iÃ§in)\n",
    "            done = False\n",
    "\n",
    "        # Collect observation and info about the current state\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, reward, done, False, info  # 'truncated' is always False\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        # Visual representation of the grid\n",
    "        grid = np.full((self.size, self.size), fill_value=' ')\n",
    "\n",
    "        # Get x and y indexes for agent_position, goal_position, and hole_position\n",
    "        # KoordinatlarÄ± al (SatÄ±r, SÃ¼tun)\n",
    "        agent_x, agent_y = self.agent_position\n",
    "        goal_x, goal_y = self.goal_position\n",
    "        hole_x, hole_y = self.hole_position\n",
    "\n",
    "        # Izgara Ã¼zerine karakterleri yerleÅŸtir\n",
    "        grid[agent_x][agent_y] = 'A'  # Agent\n",
    "        grid[goal_x][goal_y] = 'G'    # Goal\n",
    "        grid[hole_x][hole_y] = 'H'    # Hole\n",
    "\n",
    "        # GÃ¶rselleÅŸtirme\n",
    "        print(f\"\\nGrid World ({self.size}x{self.size})\")\n",
    "\n",
    "        grid[agent_x][agent_y] = 'A'\n",
    "        grid[goal_x][goal_y] = 'G'\n",
    "        grid[hole_x][hole_y] = 'H'\n",
    "\n",
    "        # Print the grid with borders\n",
    "        print(\"+---\" * self.size + \"+\")\n",
    "        for row in grid:\n",
    "            print(\"|\" + \"|\".join(f\" {cell} \" for cell in row) + \"|\")\n",
    "            print(\"+---\" * self.size + \"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡ Ã–nceki bÃ¶lÃ¼mÃ¼ doÄŸru tamamladÄ±ÄŸÄ±nÄ±zÄ± test etmek iÃ§in aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n. DoÄŸru yaptÄ±ysanÄ±z, ajanÄ±n bir bÃ¶lÃ¼mÃ¼n sonu olan hedefe veya deliÄŸe ulaÅŸana kadar ortamÄ±nÄ±zda rastgele hareket ettiÄŸini gÃ¶receksiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: {'agent': array([0, 0], dtype=int32), 'target': array([2, 2], dtype=int32)}\n",
      "Initial Info: {'distance': 4}\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 0]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 0]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 0]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 0]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 0]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([0, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 2]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([1, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -1, Done: False, Info: False\n",
      "\n",
      "Grid World (3x3)\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: {'agent': array([2, 1]), 'target': array([2, 2], dtype=int32)}, Reward: -10, Done: True, Info: False\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom environment\n",
    "env = CustomGridEnv()\n",
    "\n",
    "# Reset the environment to its initial state and get the initial observation and info\n",
    "obs, info = env.reset()\n",
    "print(\"Initial Observation:\", obs)  # Display the initial position of the agent and the target\n",
    "print(\"Initial Info:\", info)  # Display additional information such as the distance from the target\n",
    "\n",
    "# Loop through a maximum of 100 steps\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Randomly sample an action from the action space\n",
    "    obs, reward, done, info, _ = env.step(action)  # Apply the action and get the results\n",
    "    env.render()  # Render the current state of the environment to visualize the agent's position\n",
    "\n",
    "    # Print the current state, reward received, whether the episode is done, and any additional info\n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "\n",
    "    # If the episode is finished (agent reached the goal or fell into a hole), exit the loop\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BÃ¶lÃ¼m 2: DQN EÄŸitimi ğŸ¤–\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, Ã¶zel ortamÄ±nÄ±zÄ± baÅŸlatacak ve Stable Baselines3 kÃ¼tÃ¼phanesini kullanarak bir DQN ajanÄ± ile etkileÅŸim iÃ§in hazÄ±rlayacaksÄ±nÄ±z. Temel gÃ¶rev, ortamÄ±nÄ±zÄ±n kÃ¼tÃ¼phanenin gereksinimleriyle uyumlu olduÄŸundan emin olmaktÄ±r, bu da onu uygun ÅŸekilde sarmalamayÄ± iÃ§erir.\n",
    "\n",
    "#### ğŸ“ Ä°zlenecek adÄ±mlar\n",
    "\n",
    "1. ğŸ§± **Ã–zel OrtamÄ± BaÅŸlatÄ±n**: Ã¶zel ortam sÄ±nÄ±fÄ±nÄ±zÄ±n bir Ã¶rneÄŸini oluÅŸturun.\n",
    "2. ğŸ” **SB3 UyumluluÄŸunu SaÄŸlayÄ±n**: Stable Baselines3'ten `make_vec_env` fonksiyonunu kullanarak ortamÄ±nÄ±zÄ± sarÄ±n, bÃ¶ylece kÃ¼tÃ¼phanenin vektÃ¶rleÅŸtirilmiÅŸ ortam gereksinimleriyle uyumlu hale getirin.\n",
    "3. âš™ï¸ **DQN AjanÄ±nÄ± YapÄ±landÄ±rÄ±n ve EÄŸitin**: DQN ajanÄ±nÄ± uygun hiperparametrelerle kurun ve ortamÄ±nÄ±zda eÄŸitin.\n",
    "4. ğŸ“Š **EÄŸitim Ä°lerlemesini Ä°zleyin**: AjanÄ±n Ã¶ÄŸrenme ilerlemesini ve performansÄ±nÄ± zaman iÃ§inde gÃ¶zlemlemek iÃ§in gÃ¼nlÃ¼kleme ve izleme uygulayÄ±n.\n",
    "5. ğŸ’¾ **Modeli kaydedin**: EÄŸitim sonrasÄ±nda modelinizi kaydedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "EÄŸitim baÅŸlÄ±yor...\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3512 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2476        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014654569 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.0159      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 61.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2277        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017135397 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.0951      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 65.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2153        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015703658 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 69.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2111        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021787146 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 54.1        |\n",
      "-----------------------------------------\n",
      "EÄŸitim tamamlandÄ±!\n"
     ]
    }
   ],
   "source": [
    "# OrtamÄ± vektÃ¶rleÅŸtirilmiÅŸ hale getiriyoruz (SB3 iÃ§in gereklidir)\n",
    "# DummyVecEnv, tek bir ortamÄ± sarmalayarak vektÃ¶r ortamÄ± gibi davranmasÄ±nÄ± saÄŸlar\n",
    "env = DummyVecEnv([lambda: CustomGridEnv()])\n",
    "\n",
    "# PPO modelini baÅŸlatÄ±yoruz\n",
    "# 'MultiInputPolicy': GÃ¶zlem alanÄ±mÄ±z sÃ¶zlÃ¼k (Dict) olduÄŸu iÃ§in bu politikayÄ± kullanÄ±yoruz.\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "print(\"EÄŸitim baÅŸlÄ±yor...\")\n",
    "# Modeli 10.000 adÄ±m boyunca eÄŸitiyoruz\n",
    "model.learn(total_timesteps=10000)\n",
    "print(\"EÄŸitim tamamlandÄ±!\")\n",
    "\n",
    "# EÄŸitilmiÅŸ ajanÄ± kaydet\n",
    "model.save(\"ppo_custom_grid_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ® BÃ¶lÃ¼m 3: EÄŸitilmiÅŸ modelinizi kullanÄ±n\n",
    "\n",
    "EÄŸitilmiÅŸ DQN modelinizi yÃ¼kleyecek ve ortamda karar vermek iÃ§in kullanacaksÄ±nÄ±z, baÅŸlangÄ±Ã§ noktasÄ±ndan hedefe delikleri kaÃ§Ä±narak nasÄ±l gittiÄŸini gÃ¶zlemleyeceksiniz. Bu sadece eÄŸitimin etkinliÄŸini doÄŸrulamanÄ±za izin vermeyecek, aynÄ± zamanda ajanÄ±n karar verme sÃ¼recini gÃ¶rsel olarak yorumlamanÄ±za da olanak saÄŸlayacaktÄ±r.\n",
    "\n",
    "### Ä°zlenecek adÄ±mlar ğŸ“\n",
    "1. ğŸ’¾ **EÄŸitilmiÅŸ Modeli YÃ¼kleyin**: EÄŸitim aÅŸamasÄ±nda kaydedilen DQN modelini alÄ±n.\n",
    "2. ğŸ”„ **OrtamÄ± SÄ±fÄ±rlayÄ±n**: Navigasyon gÃ¶revini sÄ±fÄ±rdan baÅŸlatmak iÃ§in ortamÄ± baÅŸlatÄ±n.\n",
    "3. ğŸ§  **Navigasyon SimÃ¼lasyonunu Ã‡alÄ±ÅŸtÄ±rÄ±n**: Modeli kullanarak ortamÄ±n durumlarÄ±na dayalÄ± eylemleri tahmin edin ve ajanÄ±n attÄ±ÄŸÄ± her adÄ±mÄ± gÃ¶rselleÅŸtirin.\n",
    "4. ğŸ‘€ **Her AdÄ±mÄ± GÃ¶rselleÅŸtirin**: AjanÄ±n pozisyonunu, hedefi ve herhangi bir engeli veya deliÄŸi gÃ¶steren Ä±zgaranÄ±n basit bir gÃ¶rselleÅŸtirmesini uygulayÄ±n.\n",
    "5. ğŸ“ **Ajan DavranÄ±ÅŸÄ±nÄ± Analiz Edin**: AjanÄ±n hedefe ulaÅŸma yeteneÄŸini gÃ¶zlemleyin ve not edin ve delikleri ne kadar etkili bir ÅŸekilde kaÃ§Ä±ndÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n.\n",
    "\n",
    "Kodun bir kÄ±smÄ± zaten orada ve `# CODE HERE` yorumunu gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z her yerde kod doldurmanÄ±z gerekiyor. BaÅŸlamanÄ±z iÃ§in bazÄ± ipuÃ§larÄ± bulacaksÄ±nÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdef61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: OrderedDict({'agent': array([[0, 1]], dtype=int32), 'target': array([[2, 2]], dtype=int32)}), Reward: -1.0, Done: False, Info: [{'distance': 3, 'TimeLimit.truncated': False}]\n",
      "+---+---+---+\n",
      "|   | A |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: OrderedDict({'agent': array([[0, 2]], dtype=int32), 'target': array([[2, 2]], dtype=int32)}), Reward: -1.0, Done: False, Info: [{'distance': 2, 'TimeLimit.truncated': False}]\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: OrderedDict({'agent': array([[1, 2]], dtype=int32), 'target': array([[2, 2]], dtype=int32)}), Reward: -1.0, Done: False, Info: [{'distance': 1, 'TimeLimit.truncated': False}]\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   | A |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "State: OrderedDict({'agent': array([[0, 0]], dtype=int32), 'target': array([[2, 2]], dtype=int32)}), Reward: 10.0, Done: True, Info: [{'distance': 0, 'TimeLimit.truncated': False, 'terminal_observation': {'agent': array([2, 2]), 'target': array([2, 2], dtype=int32)}}]\n",
      "+---+---+---+\n",
      "| A |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | H | G |\n",
      "+---+---+---+\n",
      "BÃ¶lÃ¼m Sona Erdi.\n",
      "ğŸ† Tebrikler! Hedefe ulaÅŸÄ±ldÄ±.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 1. Modeli YÃ¼kle (EÄŸitim sÄ±rasÄ±nda kaydettiÄŸimiz isimle)\n",
    "model = PPO.load(\"ppo_custom_grid_agent\")\n",
    "\n",
    "obs = env.reset()  # OrtamÄ± sÄ±fÄ±rla ve ilk gÃ¶zlemi al\n",
    "\n",
    "for _ in range(200):  # Maksimum 200 adÄ±m (veya bÃ¶lÃ¼m bitene kadar)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # YOUR CODE HERE - BÃ–LÃœM 1: Tahmin ve AdÄ±m\n",
    "    # ---------------------------------------------------------\n",
    "    # Mevcut gÃ¶zleme dayanarak bir sonraki eylemi tahmin et\n",
    "    # deterministic=True, ajanÄ±n eÄŸitimden Ã¶ÄŸrendiÄŸi en iyi yolu izlemesini saÄŸlar\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # SeÃ§ilen eylemi ortamda uygula, yeni durumu ve Ã¶dÃ¼lÃ¼ al\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # OrtamÄ± gÃ¶rsel olarak temsil etmek iÃ§in boÅŸluklarla dolu 3x3 bir Ä±zgara baÅŸlat\n",
    "    grid = np.full((3, 3), fill_value=' ')\n",
    "\n",
    "    # Mevcut gÃ¶zlemi kullanarak ajan, hedef ve delik iÃ§in koordinatlarÄ± Ã§Ä±kar\n",
    "    # Not: DummyVecEnv kullandÄ±ÄŸÄ±mÄ±z iÃ§in veriler liste iÃ§indedir (batch_size=1), bu yÃ¼zden [0] kullanÄ±yoruz.\n",
    "    agent_x, agent_y = obs['agent'][0][0], obs['agent'][0][1]\n",
    "    goal_x, goal_y = obs['target'][0][0], obs['target'][0][1]\n",
    "    hole_x, hole_y = 2, 1  # DeliÄŸin konumu ortamda sabittir\n",
    "\n",
    "    # IzgarayÄ± ajanÄ±n, hedefin ve deliÄŸin konumlarÄ±yla gÃ¼ncelle\n",
    "    grid[agent_x][agent_y] = 'A'  # AjanÄ±n konumunu 'A' ile iÅŸaretle\n",
    "    grid[goal_x][goal_y] = 'G'  # Hedef konumunu 'G' ile iÅŸaretle\n",
    "    grid[hole_x][hole_y] = 'H'  # Delik konumunu 'H' ile iÅŸaretle\n",
    "\n",
    "    # Mevcut durumu, alÄ±nan Ã¶dÃ¼lÃ¼, bÃ¶lÃ¼mÃ¼n bitip bitmediÄŸini ve ek bilgileri yazdÄ±r\n",
    "    # rewards[0] ve done[0] kullanÄ±yoruz Ã§Ã¼nkÃ¼ vektÃ¶r ortamÄ± dizi dÃ¶ndÃ¼rÃ¼r\n",
    "    print(f\"State: {obs}, Reward: {rewards[0]}, Done: {done[0]}, Info: {info}\")\n",
    "\n",
    "    # IzgarayÄ± konsolda gÃ¶rsel olarak yazdÄ±r\n",
    "    print(\"+---\" * 3 + \"+\")  # IzgaranÄ±n Ã¼st sÄ±nÄ±rÄ±\n",
    "    for row in grid:\n",
    "        print(\"|\" + \"|\".join(f\" {cell} \" for cell in row) + \"|\")  # Her satÄ±rÄ± hÃ¼creler '|' ile ayrÄ±lmÄ±ÅŸ ÅŸekilde yazdÄ±r\n",
    "        print(\"+---\" * 3 + \"+\")  # Her satÄ±rdan sonra ayÄ±rÄ±cÄ± sÄ±nÄ±r\n",
    "\n",
    "    # AdÄ±mlarÄ± gÃ¶zle takip edebilmek iÃ§in kÄ±sa bir bekleme sÃ¼resi (Opsiyonel ama Ã¶nerilir)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # YOUR CODE HERE - BÃ–LÃœM 2: BitiÅŸ KontrolÃ¼\n",
    "    # ---------------------------------------------------------\n",
    "    # EÄŸer bÃ¶lÃ¼m bittiyse (hedefe ulaÅŸÄ±ldÄ± veya ajan deliÄŸe dÃ¼ÅŸtÃ¼)\n",
    "    if done[0]:\n",
    "        print(\"BÃ¶lÃ¼m Sona Erdi.\")\n",
    "        if rewards[0] > 0:\n",
    "            print(\"ğŸ† Tebrikler! Hedefe ulaÅŸÄ±ldÄ±.\")\n",
    "        else:\n",
    "            print(\"â˜ ï¸ Ajan deliÄŸe dÃ¼ÅŸtÃ¼.\")\n",
    "        \n",
    "        # OrtamÄ± sÄ±fÄ±rla (veya dÃ¶ngÃ¼yÃ¼ kÄ±r)\n",
    "        obs = env.reset()\n",
    "        break # Genellikle tek bir gÃ¶sterimden sonra Ã§Ä±kmak daha temizdir.\n",
    "    # ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  ArtÄ±k ajanÄ±nÄ±zÄ±n ortamda hareket ettiÄŸini gÃ¶rmelisiniz.\n",
    "\n",
    "EÄŸer ajan **belirli eylemleri tekrarlayarak takÄ±lÄ±p kalÄ±rsa** veya hedefe ulaÅŸamazsa, muhtemelen **yeterince uzun eÄŸitilmediÄŸi** iÃ§indir.\n",
    "\n",
    "EÄŸitim adÄ±m sayÄ±sÄ±nÄ± artÄ±rmayÄ± deneyin ve modeli yeniden eÄŸitin â€” daha uzun eÄŸitim genellikle ajanÄ±n daha iyi bir politika Ã¶ÄŸrenmesine yardÄ±mcÄ± olur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
